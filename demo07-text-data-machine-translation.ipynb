{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Hyc8nBFB8h"
   },
   "source": [
    "# Goal: Work with Text Data and Explore Neural Machine Translation Models\n",
    "In this demo, we will explore the process of working with text data for Neural Machine Translation from German to English.\n",
    "\n",
    "In the previous demos, we worked with image classification datasets that were already well suited as inputs to neural networks (i.e. floating point tensors). On the other hand, text data needs to be properly parsed and formatted for DNN inference and training. We'll go through the process of transforming our text into numerical representations. Finally, we will build an encoder-decoder model to translate German sentences to their English equivalent. We will follow the design choices of two papers ([Sutskever et al. 2014](https://arxiv.org/abs/1409.3215) and [Cho et al. 2014](https://arxiv.org/abs/1406.1078l)). \n",
    "\n",
    "### Text Data\n",
    "The process for working with text data:\n",
    "- import our text as string in Python\n",
    "- tokenize the strings (i.e. split the strings)\n",
    "- build a vocabulary (associate each token with an index)\n",
    "\n",
    "### Neural Machine Translation\n",
    "- Encoder-Decoder for Sequence to Sequence (German to English)\n",
    "\n",
    "### Some Useful Resources:\n",
    "\n",
    "- [PyTorch's Tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)\n",
    "- [Teal's Transformer notebook](https://github.com/rtealwitter/dl-demos/blob/main/demo07-transformers.ipynb)\n",
    "- [D2L Chapter 8 & 9](https://d2l.ai/chapter_recurrent-neural-networks/text-sequence.html)\n",
    "- [Sequence to Sequence Notebook](https://github.com/nitarshan/sequence-to-sequence-learning/blob/master/Sequence%20to%20Sequence%20Learning%20for%20Translation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfo6DBbbFZni"
   },
   "source": [
    "# Imports\n",
    "\n",
    "Let's import some useful libraries. You will see our classic import such as `torch` but also [`torchtext`](https://pytorch.org/text/stable/index.html) and [`spacy`](https://spacy.io/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bXDugENCys2",
    "outputId": "d4e44025-7e91-475b-eea8-0580210582e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import Counter # counter from python standard library\n",
    "import spacy                    # tokenization step \n",
    "\n",
    "import torchtext\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "import io\n",
    "import random\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "# let's make sure we have a GPU available\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5q5zfy6XT9w"
   },
   "source": [
    "In order to parse the sentence data we will be working with, we need to download the german and english dependencies from `spacy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIf0UJVoGBxp"
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlPiMz1rFcN3"
   },
   "source": [
    "# Get German / English Translation Dataset\n",
    "At this point, we have our imports and are ready to import our dataset. The `multi30k` dataset has several language so we'll just focus on german (`de`) and english (`en`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FQgWsRfEKVtX",
    "outputId": "997541b8-f740-442c-d928-c0eb80c04449"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 637k/637k [00:00<00:00, 31.7MB/s]\n",
      "100%|██████████| 569k/569k [00:00<00:00, 45.1MB/s]\n",
      "100%|██████████| 24.7k/24.7k [00:00<00:00, 7.61MB/s]\n",
      "100%|██████████| 21.6k/21.6k [00:00<00:00, 10.7MB/s]\n",
      "100%|██████████| 22.9k/22.9k [00:00<00:00, 7.29MB/s]\n",
      "100%|██████████| 21.1k/21.1k [00:00<00:00, 6.75MB/s]\n"
     ]
    }
   ],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U9T0s0PYWPv"
   },
   "source": [
    "This downloads and extracts the text data within the `.data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YmUp30yMKfLk",
    "outputId": "99b59778-8275-45a2-885b-59434b3da43c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_2016_flickr.de\ttest_2016_flickr.en.gz\ttrain.en     val.de.gz\n",
      "test_2016_flickr.de.gz\ttrain.de\t\ttrain.en.gz  val.en\n",
      "test_2016_flickr.en\ttrain.de.gz\t\tval.de\t     val.en.gz\n"
     ]
    }
   ],
   "source": [
    "!ls .data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyKmoBH9YbNA"
   },
   "source": [
    "Now, let's examine the first 10 sentences of both the English and German sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6S4kwdQYlur",
    "outputId": "472a3a16-cf27-4ea1-ec52-8f93c923c3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German : Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
      "English: Two young, White males are outside near many bushes.\n",
      "--------------------------------------------------\n",
      "German : Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
      "English: Several men in hard hats are operating a giant pulley system.\n",
      "--------------------------------------------------\n",
      "German : Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
      "English: A little girl climbing into a wooden playhouse.\n",
      "--------------------------------------------------\n",
      "German : Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
      "English: A man in a blue shirt is standing on a ladder cleaning a window.\n",
      "--------------------------------------------------\n",
      "German : Zwei Männer stehen am Herd und bereiten Essen zu.\n",
      "English: Two men are at the stove preparing food.\n",
      "--------------------------------------------------\n",
      "German : Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
      "English: A man in green holds a guitar while the other man observes his shirt.\n",
      "--------------------------------------------------\n",
      "German : Ein Mann lächelt einen ausgestopften Löwen an.\n",
      "English: A man is smiling at a stuffed lion\n",
      "--------------------------------------------------\n",
      "German : Ein schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\n",
      "English: A trendy girl talking on her cellphone while gliding slowly down the street.\n",
      "--------------------------------------------------\n",
      "German : Eine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\n",
      "English: A woman with a large purse is walking by a gate.\n",
      "--------------------------------------------------\n",
      "German : Jungen tanzen mitten in der Nacht auf Pfosten.\n",
      "English: Boys dancing on poles in the middle of the night.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lines_en = open(\".data/train.en\", 'r', encoding=\"utf-8\").read().splitlines()\n",
    "lines_de = open(\".data/train.de\", 'r', encoding=\"utf-8\").read().splitlines()\n",
    "\n",
    "NUM_LINES = 10\n",
    "for i in range(NUM_LINES):\n",
    "  print(\"German :\", lines_de[i])\n",
    "  print(\"English:\", lines_en[i])\n",
    "  print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk9WyergLD-0"
   },
   "source": [
    "Each setences is simply a Python string. So we need to convert this string into a tensor that works with our neural networks. The first step of this process is tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTGnSzlaFfx0"
   },
   "source": [
    "# Tokenize\n",
    "\n",
    "Tokenizing is the process of breaking down our sentences into chunks or units that we can work with. In this notebook, **we will be tokenizing at the word level**. However, the granularity at which you tokenize depends upon the problem you are trying to solve. For example, you could also tokenize at the character level. \n",
    "\n",
    "\n",
    "Now, we could do this ourselves by parsing and splitting the strings. But there are a few things to keep in mind such as capitalization, special characters, and punctuation. Luckily, there are existing libraries that handle tokenization for us such as `spacy`. \n",
    "\n",
    "Let's repeat our imports just to see which ones we are making use of. Let's instatiate both the German and English tokenizers. `spacy` works well with `torchtext`. Note that we specify the language to the `get_tokenizer()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VpGpr8_RLTFD"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from  torchtext.vocab import vocab\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm') \n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OtqZMCmadA7"
   },
   "source": [
    "The first step is to iterate through our corpus and count the occurences of each token. Let's use `Counter` from `collections` which is part of the Python Standard Library (it's similar to a Python dictionary). Let's test out the `Counter` on the training dataset for the English sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "l4QPPqtTLxMC"
   },
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "with io.open('.data/train.en', encoding='utf8') as f:\n",
    "  for string in f:\n",
    "    counter.update(en_tokenizer(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJwa9jTVbOLj"
   },
   "source": [
    "Let's see the number of unique tokens we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kegG-uCLMCwR",
    "outputId": "bb296c8d-291f-4f1f-fc4d-370def5f8249"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10834\n"
     ]
    }
   ],
   "source": [
    "print(len(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-jabNVYbTYO"
   },
   "source": [
    "Looks like we have 10834 unique words for out dataset. Now, let's print out the top 10 most frequent words in the Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6eU72OdbT0g",
    "outputId": "4e3cdca4-0f21-4a57-8cd7-ccd215a8150a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 31707)\n",
      "('\\n', 29000)\n",
      "('.', 27623)\n",
      "('A', 17458)\n",
      "('in', 14847)\n",
      "('the', 9923)\n",
      "('on', 8019)\n",
      "('is', 7524)\n",
      "('and', 7378)\n",
      "('man', 7359)\n"
     ]
    }
   ],
   "source": [
    "sorted_counter = sorted(counter.items(), key=lambda kv: -kv[-1])\n",
    "for i in range(10):\n",
    "  print(sorted_counter[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6goQFt9cjAU"
   },
   "source": [
    "So, we can see the most frequent words found in our dataset. Note that the special character `\\n` is the second most frequent! At this point, we can convert our counter into a vocab. Each key in our `Counter` dictionary is now a token. Our vocabulary will map a token to an index in our vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lYx2LuN6Fjf4"
   },
   "source": [
    "# Building a Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acbWMbSWct2K"
   },
   "source": [
    "Here, we use the `vocab` class from `torchtext` to handle this for us. We will also be passing in some extra, special tokens:\n",
    "\n",
    "- `<unk>` for when we encounter a token that wasn't in our vocab (like a misspelled word)\n",
    "- `<pad>` we can add this token to artifically increase the length of our sentence. This will be helpful for training efficiency.\n",
    "- `<bos>` signifies the beginning of a sentence\n",
    "- `<eos>` signifies the end of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UlsfykfrMfHg"
   },
   "outputs": [],
   "source": [
    "en_vocab = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a0nIgSf3dWMy"
   },
   "source": [
    "Now, let's see how to use our `vocab` object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVp54jHkMx4l"
   },
   "source": [
    "# STOI: Convert a token to an index in our vocab.\n",
    "\n",
    "We can simply index into our `vocab` object with a token present in our training dataset. I'm guessing the word `cat` was in our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xP0txhrfMqSc",
    "outputId": "2f963d5f-7ba4-45e9-b703-206efe31b72c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1513"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[\"cat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxM0FKq-dlQD"
   },
   "source": [
    "So, the token `cat` will always be mapped to the index `1513`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQKvqXVnM2P0"
   },
   "source": [
    "# ITOS: Convet an index in our vocab to its token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rjKk99udwHb"
   },
   "source": [
    "We can reverse this process too using the `lookup_token()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "xh6-Dn0vM6ar",
    "outputId": "d1b5bd56-f9aa-453a-ba3e-adff79611b93"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab.lookup_token(1513)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDXBDaZHd0Tv"
   },
   "source": [
    "Here's another example for the exclamation punctuation mark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Newfk7p2d3RF",
    "outputId": "ff1ca55b-e3b5-4ed3-e73a-ab51baca301a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2391\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab['!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMigzLAyd9di",
    "outputId": "32e05b26-35df-4360-df22-b8435bf15b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab.lookup_token(2391))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYqqlvH4d_qm"
   },
   "source": [
    "Now, let's check out the index in our vocab for the special tokens we instatiated before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxTvwbxFNFPQ",
    "outputId": "af495dd5-3463-4381-e953-be34db4409c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab['<unk>'])\n",
    "print(en_vocab['<pad>'])\n",
    "print(en_vocab['<bos>'])\n",
    "print(en_vocab['<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFMtus_keD_D"
   },
   "source": [
    "So, these are just the first four indices in our vocab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBgzlPxdeJSE"
   },
   "source": [
    "Now, the reason we have the `<unk>` token is to ensure that if we encounter a word (token) that we haven't seen before, we can map it gracefully to some index in our vocab. Let's see what happens now if we try to find the index of a word that doesn't exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "OHoGs5GWN2IC",
    "outputId": "44e727a8-3368-4d44-c039-76617500db4a"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e66c8a6f5a30>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0men_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fjflsj'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchtext/vocab/vocab.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0massociated\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \"\"\"\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Token fjflsj not found and default index is not set"
     ]
    }
   ],
   "source": [
    "en_vocab['fjflsj']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_C0jJRpeU6T"
   },
   "source": [
    "In order to solve this edge case, we can set the `<unk>` token as the default token in our vocab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3F9cykZQeaaM"
   },
   "outputs": [],
   "source": [
    "en_vocab.set_default_index(en_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VESsmv4eifX"
   },
   "source": [
    "Now, let's see what index we get for a couple of misspelled words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E9S_xDa5elTX",
    "outputId": "c4fa809e-2354-402f-dc9a-29a8ee7d1ad8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab['fjflsj']) # some random string\n",
    "print(en_vocab['appel']) # apple misspelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puD0pJ44esQD"
   },
   "source": [
    "They both get mapped to the unknown token! Let's wrap this all up in a function and build our vocab for both German and English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "zt6tfAZvN-yM"
   },
   "outputs": [],
   "source": [
    "def build_vocab(filepath, tokenizer):\n",
    "  counter = Counter()\n",
    "  with io.open(filepath, encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "      counter.update(tokenizer(string_))\n",
    "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab('.data/train.de', de_tokenizer)\n",
    "de_vocab.set_default_index(de_vocab['<unk>'])\n",
    "en_vocab = build_vocab('.data/train.en', en_tokenizer)\n",
    "en_vocab.set_default_index(en_vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcrCfQT1e94w"
   },
   "source": [
    "Let's check the size of our vocab for both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-T8szu39OH9x",
    "outputId": "4e1715ed-6425-4713-cc5b-c4e94bd73944"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19215, 10838)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(de_vocab), len(en_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWlD7DBOFm4k"
   },
   "source": [
    "# Convert Sentences to Torch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tPX_SRNfArg"
   },
   "source": [
    "At this stage, we can now map tokens to indices which are integers. We are one step closer to being able to pass text data into our neural networks. The following function just maps each token to their respective index for each vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IULl6tgSOVPI"
   },
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuRqYwqROjD-",
    "outputId": "d69c07de-6345-4659-e165-034481be9a03"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17]),\n",
       " tensor([ 4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBu5NTsvfQju"
   },
   "source": [
    "As a sanity check, let's convert each index back to its token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C31QQvsMfW39",
    "outputId": "aff58047-4612-431c-f672-f76857a79e50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche . \n",
      " Two young , White males are outside near many bushes . \n",
      " "
     ]
    }
   ],
   "source": [
    "de_sentence, en_sentence = train_data[0]\n",
    "for idx in de_sentence:\n",
    "  print(de_vocab.lookup_token(idx), end=' ')\n",
    "\n",
    "for idx in en_sentence:\n",
    "  print(en_vocab.lookup_token(idx), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkxqMl8AfrcO"
   },
   "source": [
    "Now, we are ready to create our DataLoaders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ve1kG6EUFpzy"
   },
   "source": [
    "# DataLoader\n",
    "\n",
    "Here's what our dataloader should do:\n",
    "- Returns a batch of data\n",
    "- Add `<bos>` token to the beginning of each sentence\n",
    "- Add `<eos>` token to the end of each sentence\n",
    "- Each sentence in the batch to have the same length `<pad>`\n",
    "\n",
    "We want each sentence to have the same length so it's straightforward to pass in batches of sentences to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "go8zIrswPBEg"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0)) #BOS Sentence EOS - German\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0)) #BOS Sentence EOS - English\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch\n",
    "\n",
    "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2UYpLk7FryR"
   },
   "source": [
    "# Explore Torchified Sentence Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PagpKIIwf5F7"
   },
   "source": [
    "Let's explore the data that's returned by our Dataloader. This data will be input to our Encoder-Decoder Neural Machine Translation Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPd191HfPZqa",
    "outputId": "7813e3f2-0404-45fc-ab5f-fa786751f350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 128]) torch.Size([33, 128])\n"
     ]
    }
   ],
   "source": [
    "src_de, trg_en = next(iter(train_iter))\n",
    "print(src_de.shape, trg_en.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY8MKOXmgEA-"
   },
   "source": [
    "The size of our torch objects are: `length of sentence` $\\times$ `batch size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJpN9_E9gMqK"
   },
   "source": [
    "We set the batch size to 128 and each sentence in a batch will have the same length. Note that the sentence size might `differ` between the languages. Let's print out our first German sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKrXW2FAPsFE",
    "outputId": "b4e479c4-a2ba-4355-8062-68f567d0ddb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   2,   24, 1072, 1674, 2843,  732,   11,   68, 2587,   52,   53,   12,\n",
      "        2884, 1521,  961, 1277,   47, 5235,  282,   16,   17,    3,    1,    1,\n",
      "           1,    1,    1,    1,    1,    1,    1])\n"
     ]
    }
   ],
   "source": [
    "print(src_de[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0wjoVLOgZnQ"
   },
   "source": [
    "Let's print the values of our special tokens again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ENj-0u90gdSD",
    "outputId": "a907a31f-e171-4d87-9b97-7583799c53ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab['<bos>'])\n",
    "print(en_vocab['<eos>'])\n",
    "print(en_vocab['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqSLcDwhgk7S"
   },
   "source": [
    "So, we can see our sentence starts with the `<bos>` token, ends with several `<pad>` tokens. The actual sentences itself ends with the `<eos`> token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX8lB0wvhGoR"
   },
   "source": [
    "Let's convert our sentence to their tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnv5psgnP6vV",
    "outputId": "c7b38288-7fd6-4910-ea6c-a4a3ec17d839"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> Ein weiß gekleideter Baseballspieler rutscht in die Base , während der grau gekleidete Spieler ihn zu berühren versucht . \n",
      " <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "for word in src_de[:,0]:\n",
    "  print(de_vocab.lookup_token(word), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlZJ7Zz7hKEb"
   },
   "source": [
    "Since there is a one to one mapping between the German and English sentences, let's go ahead and print the first English sentence as well. It should be the corresponding English sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXKG2EZpQaKL",
    "outputId": "a5a6203b-0f41-480f-a098-bdb8dfef6bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> A baseball player in white is sliding into the base while the player in gray is trying to tag him . \n",
      " <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> "
     ]
    }
   ],
   "source": [
    "for word in trg_en[:,0]:\n",
    "  print(en_vocab.lookup_token(word), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_KgNg-rhapM"
   },
   "source": [
    "Finally, let's print the first 10 sentences. Each sentence will be a column that starts with the index 2 (`<bos>` token) and is padded to be the same length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6NCPe2wQNN3",
    "outputId": "ec76680d-e73f-418b-87b7-0b30691aee17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2],\n",
      "        [   24,    71,   408,    24,    71,    24,    24,    24,    24,    24],\n",
      "        [ 1072,    72, 14316,   251,    72,  2843,   251,  5434,    31,    97],\n",
      "        [ 1674,    19,    36,    19,    11,   463,    11,  1192,   116,    98],\n",
      "        [ 2843,   984,    37,  1006,    32,    22,    32,   213,   489,    19],\n",
      "        [  732,   274,  1759,  3961,   278, 18241,  5252,   852,    11,    37],\n",
      "        [   11,    58,    47,    22,    34,   827,    36,  2398,    12,  6561],\n",
      "        [   68,   490,  2544,   283,    49,  3505,    32,  2241, 11398,    35],\n",
      "        [ 2587,  1400,    16,  3385,    58,    52,  5929,    68,    52,   117],\n",
      "        [   52,    52,    17,    36,  7355,   233,    33,   280,    36,    22],\n",
      "        [   53,    61,     3,    22,   320,    47,  1255,   681,    32,   391],\n",
      "        [   12,    64,     1, 17631,    16,    37,     9,    52,  2853,   295],\n",
      "        [ 2884, 14690,     1,    16,    17,  2587,   307,   852,  3913,  7962],\n",
      "        [ 1521,  6200,     1,    17,     3,    47,    16,   449,    37,    16],\n",
      "        [  961,  2059,     1,     3,     1,  5167,    17,  1375,    73,    17],\n",
      "        [ 1277,    42,     1,     1,     1,    52,     3,   593,   410,     3],\n",
      "        [   47,    16,     1,     1,     1,    53,     1,  3190,    16,     1],\n",
      "        [ 5235,    17,     1,     1,     1,    12,     1,    16,    17,     1],\n",
      "        [  282,     3,     1,     1,     1, 17435,     1,    17,     3,     1],\n",
      "        [   16,     1,     1,     1,     1,    55,     1,     3,     1,     1],\n",
      "        [   17,     1,     1,     1,     1,  7966,     1,     1,     1,     1],\n",
      "        [    3,     1,     1,     1,     1,  5210,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,   463,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,    16,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,    17,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     3,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "print(src_de[:,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhATK2pZFviT"
   },
   "source": [
    "# Setting up our Neural Machine Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7sgO0GKnhlu8"
   },
   "source": [
    "We are ready to setup our Neural Machine Translation model. We will be using an Encoder-Decoder model where both the components will be RNNs (in particular, [GRUs](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html)). Remember, we are working with sequence data so each token will be input sequentially to our GRU units. At a high level: our model looks like this:\n",
    "\n",
    "![](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/encoder-decoder.svg)\n",
    "\n",
    "### Prediction\n",
    "Let's first discuss how prediction would work assuming we've trained the model:\n",
    " \n",
    "The overall idea is that we input our German sentence to the encoder which will output a final hidden state. This hidden state essentially summarizes and represents the input German sentence in some latent space. For the Decoder, we will set the initial hidden state to the final hidden state from the encoder. We will then feed in the `<bos>` token. Then, the Decoder will output a hidden state. We will transform this hidden state to a distribution over the english vocab (a tensor of length 10838). We can them sample from this distribution to obtain the next token. Now, the updated hidden state and the new token will be fed into our Decoder and we will repeat this process until we sample the `<eos>` token. \n",
    "\n",
    "\n",
    "### Training\n",
    "Now, let's think about training. We have both the German sentence as well as their translation in English. We will use the idea of Teacher-Forcing. Here's a diagram from the textbook that shows this process for English to French:\n",
    "\n",
    "![](https://raw.githubusercontent.com/d2l-ai/d2l-en/master/img/seq2seq.svg)\n",
    "\n",
    "At training time, we input our sentence as usual. For the decoder, we input the translated sentence but chop off the final token. Our ground truth predication will be the translated sentence shifted over by one token. Each token will produce a hidden state that is tranformed to a distribution over our language. We can then use the Cross Entropy loss over this distribution and the ground truth token that should be sampled.\n",
    "\n",
    "\n",
    "### Embedding\n",
    "There is a final detail we need to take care of. Each token has been converted to an index into our vocab. The final transformation we will apply is a transformation of our index to a latent space called an embedding. We get to choose the size of this latent space. An example will show how this works. Let's imagine we have a vocab of just 10 words and we would like to embed these tokens into a latent space of size 4. Here's how we can do it in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0YspJzfk2Ep",
    "outputId": "a09aebb2-5c5f-4ca4-dd95-69b8580d3395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(10, 4)\n",
      "tensor([[-0.3741,  0.6554, -1.7362,  0.0513],\n",
      "        [ 1.5785,  1.5529,  0.9754,  0.3551],\n",
      "        [-1.9181,  1.2132, -0.4472,  0.6887],\n",
      "        [ 0.1751, -0.4901,  0.1189,  0.6863],\n",
      "        [-0.7948, -2.2631, -3.5633,  1.3887],\n",
      "        [-0.9918, -0.6075, -0.0085, -0.5732],\n",
      "        [-0.1582, -0.4766, -0.7730, -0.3657],\n",
      "        [ 0.2304,  0.4892,  0.0140,  0.4349],\n",
      "        [ 0.0244, -1.2378, -0.6552,  0.9723],\n",
      "        [-1.3659,  1.7206, -1.0083,  0.5567]])\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 4)\n",
    "print(embedding)\n",
    "print(embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8F4qtmQlvAG"
   },
   "source": [
    "Each token will index into our embedding and will be represented as a tensor of size 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWmWGYL2k5XS"
   },
   "source": [
    "Suppose we want to transform the `<bos>` token (which has an index of 2 in our vocabs). We can do perform the embedding as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5249ozohl9w_",
    "outputId": "01eb951f-7b3d-4055-d030-73be405c25e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.9181,  1.2132, -0.4472,  0.6887]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding(torch.tensor([[en_vocab['<bos>']]]))) ## equivalent to embedding(torch.tensor([[2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNboAjf0mGcw"
   },
   "source": [
    "This is just the fourth row in our embedding matrix. Note that we can find the local derivative of our embedding. This means we can train embeddings! Through the training process, similar words can be transformed to similar embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kth_Bbw6mZHb"
   },
   "source": [
    "## Setting up the Model\n",
    "Now, we are ready to build our encoder-decoder model. We need to first embed our indices to a latent space and process them through the GRUs. We will be use a bidirectional GRU for the encoder in order to process the source German sentence in both directions ([Sutskever et al. 2014](https://arxiv.org/abs/1409.3215) found this helpful). Doing so increases the depth of the GRU by a factor of two.\n",
    "\n",
    "We will use:\n",
    "\n",
    "- an embedding space of size 128\n",
    "- a hidden state size of 256\n",
    "- 4-layer GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "D8WohHKuRAPL"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Seq2Seq, self).__init__()\n",
    "    SRC_EMB_SIZE = 128\n",
    "    TRG_EMB_SIZE = 128\n",
    "    H_SIZE = 256\n",
    "    LAYERS = 4\n",
    "\n",
    "    self.src_emb = nn.Embedding(len(de_vocab), 128) # embedding for german tokens\n",
    "    self.trg_emb = nn.Embedding(len(en_vocab), 128) # embedding for english tokens\n",
    "\n",
    "    self.encoder = nn.GRU(SRC_EMB_SIZE, H_SIZE, LAYERS//2, bidirectional=True) # encoder\n",
    "    self.decoder = nn.GRU(TRG_EMB_SIZE, H_SIZE, LAYERS) # decoder\n",
    "    self.to_trg = nn.Linear(H_SIZE, len(en_vocab)) # transform the hidden states to a probability distribution\n",
    "\n",
    "  def forward(self, src_ids, trg_ids):\n",
    "    src_emb = self.src_emb(src_ids) # embed the german tokens\n",
    "    enc_output, enc_hidden = self.encoder(src_emb) # process through the bidirectional GRU\n",
    "\n",
    "\n",
    "    trg_emb = self.trg_emb(trg_ids) # embed the english tokens (just for training)\n",
    "    dec_output, dec_hidden = self.decoder(trg_emb, enc_hidden) # process target through decoder GRU, note the hidden state \n",
    "\n",
    "    logits = self.to_trg(dec_output) # convert each hidden state to a distribution over the english language\n",
    "    preds = F.log_softmax(logits, dim=2) # convert to a normalized dist.\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3afPIihnZCf"
   },
   "source": [
    "Let's perform a sanity check and make sure things work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65z9JWYsUDHA",
    "outputId": "9b1a02ed-db3f-4361-cb31-474457959d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 128]) torch.Size([31, 128])\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq()\n",
    "src_de, trg_en = next(iter(train_iter))\n",
    "print(src_de.shape, trg_en.shape)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  out = model(src_de, trg_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5X-1aSh1nc_d"
   },
   "source": [
    "Let's analyze the output shape of our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oze_dFknUTDL",
    "outputId": "bea87866-f209-443c-bd5f-527c65badd99"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 128, 10838])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQBYUCSenf0O"
   },
   "source": [
    "We see `torch.Size([31, 128, 10838])`:\n",
    "\n",
    "- 31 is the output size of our english sentence\n",
    "- 128 is the batch size\n",
    "- each of the 31 outputs maps to a distribution over the english langauge (which had a size of 10838 tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGBN51A4FyAD"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGY8tl-Cns2S"
   },
   "source": [
    "Let's train this model. We will use the Adam optimizer. Importantly, we don't want the `<pad>` token to contribute to our loss, so let's handle that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "g6fPxWaMUiBy"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "PAD_IDX = en_vocab['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCCCYMQSoEkX"
   },
   "source": [
    "Note that we use teacher forcing. For the input to the decoder, we chop off the final token (`trg[:-1]`). For the loss function, we shift over the target sentence (`trg[1:]`). Recall that RNN models are prone to both gradient vanishing and exploding. Let's clip the gradients if they are too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hzZnpb11Uzie",
    "outputId": "7395ff9e-6bf0-419f-d2f4-b3883b4daeb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 4.926530113304239\n",
      "Epoch 1: 4.003010595422484\n",
      "Epoch 2: 3.384852639378955\n",
      "Epoch 3: 3.013778653964072\n",
      "Epoch 4: 2.7345615998238717\n",
      "Epoch 5: 2.5076538938782815\n",
      "Epoch 6: 2.3079120944775147\n",
      "Epoch 7: 2.127656368957217\n",
      "Epoch 8: 1.9613322502715997\n",
      "Epoch 9: 1.8055939700634993\n",
      "Epoch 10: 1.6560819637407815\n",
      "Epoch 11: 1.5185737315778691\n",
      "Epoch 12: 1.388288369262796\n",
      "Epoch 13: 1.2640819502296952\n",
      "Epoch 14: 1.1521452927904507\n",
      "Epoch 15: 1.0449961833491725\n",
      "Epoch 16: 0.9463206175140347\n",
      "Epoch 17: 0.8578489257375574\n",
      "Epoch 18: 0.7723013556476207\n",
      "Epoch 19: 0.6927754246190782\n",
      "Epoch 20: 0.6184131293044741\n",
      "Epoch 21: 0.5528884221541199\n",
      "Epoch 22: 0.4944252854926996\n",
      "Epoch 23: 0.4407538074491308\n",
      "Epoch 24: 0.39049183705304685\n",
      "Epoch 25: 0.3448183296272933\n",
      "Epoch 26: 0.30316996797591056\n",
      "Epoch 27: 0.2650890880087924\n",
      "Epoch 28: 0.23480227569914075\n",
      "Epoch 29: 0.2096506143850377\n",
      "Epoch 30: 0.18725315563479183\n",
      "Epoch 31: 0.17502348755162192\n",
      "Epoch 32: 0.16177015991200436\n",
      "Epoch 33: 0.14989684728930175\n",
      "Epoch 34: 0.1383056765020156\n",
      "Epoch 35: 0.12525089761234065\n",
      "Epoch 36: 0.11209428707539773\n",
      "Epoch 37: 0.0999004048200956\n",
      "Epoch 38: 0.09157855086652193\n",
      "Epoch 39: 0.08771323279727923\n"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "model.train()\n",
    "for epoch in range(40):\n",
    "  epoch_loss = 0\n",
    "  for _, (src, trg) in enumerate(train_iter):\n",
    "    src, trg = src.cuda(), trg.cuda()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    #teacher-forcing\n",
    "    output = model(src, trg[:-1]) # removing the last token\n",
    "    output = output.view(-1, output.shape[-1])\n",
    "\n",
    "    loss = criterion(output, trg[1:].view(-1)) # removing the first token\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1) # grad clipping\n",
    "\n",
    "    optimizer.step() # update our weights\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "  print(f\"Epoch {epoch}:\", epoch_loss / len(train_iter))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUEhL8a7F1ls"
   },
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WsqtA68qsiHC"
   },
   "source": [
    "At this point, we are ready to take a never-before-seen German sentence and translate it. We will write a fucntion below that performs the prediction as we noted above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ZuUve_SNY-dD"
   },
   "outputs": [],
   "source": [
    "def greedy_sample(src, trg, idx):\n",
    "  # push batch to the GPU\n",
    "  src, trg = src.cuda(), trg.cuda()\n",
    "  model.eval()\n",
    "\n",
    "  # isolate the sentence using the idx parameter\n",
    "  sample = src[:, idx:idx+1] # choosing a sentence in german\n",
    "  gt = trg[:, idx:idx+1]     # english translation from the dataset\n",
    "  input = [de_vocab.lookup_token(sample[i,0]) for i in range(len(sample))]\n",
    "  input = input[1:input.index(\"<eos>\")]\n",
    "  gt = [en_vocab.lookup_token(gt[i, 0]) for i in range(len(gt))]\n",
    "  gt = gt[1:gt.index(\"<eos>\")]\n",
    "\n",
    "  # print both the input german sentence and the ground truth\n",
    "  print(\"input:\", \" \".join(input))\n",
    "  print(\"gt   :\", \" \".join(gt))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # perform model prediction (inference)\n",
    "  output = []\n",
    "  # embed source sentence\n",
    "  src_sen_emb = model.src_emb(sample)\n",
    "  # process through the encoder\n",
    "  enc_output, enc_hidden = model.encoder(src_sen_emb)\n",
    "  dec_hidden = enc_hidden \n",
    "\n",
    "  # start our sentence with the beginning of sequence token\n",
    "  out = de_vocab['<bos>']\n",
    "\n",
    "  # sample for 40 tokens\n",
    "  for i in range(40):\n",
    "    \n",
    "    # embed the token\n",
    "    trg_sen_emb = model.trg_emb(torch.tensor([[out]], device='cuda'))\n",
    "    dec_output, dec_hidden = model.decoder(trg_sen_emb, dec_hidden) #passing in hidden state from encoder\n",
    "    preds = F.log_softmax(model.to_trg(dec_output), dim=2) \n",
    "    out = preds.argmax(2) # choosing the word with the highest probability\n",
    "    output.append(en_vocab.lookup_token(out.item())) # convert to token\n",
    "  if \"<eos>\" in output:\n",
    "    output = output[:output.index(\"<eos>\")]\n",
    "  print(\"pred:\", \" \".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1bjC3pptGi1"
   },
   "source": [
    "Let's try out our trained Neural Machine Translation Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHk1-B0zZ0rY",
    "outputId": "4db70c61-e2ba-47fe-ed09-eb1eabe8a1d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([34, 128]) torch.Size([36, 128])\n"
     ]
    }
   ],
   "source": [
    "src, trg = next(iter(test_iter))\n",
    "print(src.shape, trg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nM5focw-Z-BD",
    "outputId": "16a9de62-88fe-442a-e32b-80bff8e39739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Zwei Männer in Anzügen unter einem Regenschirm vor einem Graffiti . \n",
      "\n",
      "gt   : Two men in suits under an umbrella and in front of graffiti . \n",
      "\n",
      "pred: Two men in military clothing are outside in a front of truck . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnd_idx = random.randint(0,128)\n",
    "greedy_sample(src, trg, rnd_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VWeKAR_tMuO"
   },
   "source": [
    "So, we can see it's performing *some* translation, but maybe not the best. Let's try a few more samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqHFb9ITwYsh",
    "outputId": "48fce7bc-94df-458f-9ab1-4626e8ef6fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Ein junges Paar sitzt auf dem Gehsteig und entspannt gemeinsam . \n",
      "\n",
      "gt   : A young couple sits on the sidewalk and relaxes together . \n",
      "\n",
      "pred: A young couple is sitting on the sidewalk wearing camping or clothing . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnd_idx = random.randint(0,128)\n",
    "greedy_sample(src, trg, rnd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3L-DMvPtiSK",
    "outputId": "e9812728-a840-4011-c229-e44815630bce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Leute gehen auf einer befestigten <unk> Straße , umgeben von <unk> Händlern . \n",
      "\n",
      "gt   : People are walking on a paved slope surrounded by Chinese vendors . \n",
      "\n",
      "pred: People walk down a busy crowded street with many stone lights . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnd_idx = random.randint(0,128)\n",
    "greedy_sample(src, trg, rnd_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jdqz08OtlYQ",
    "outputId": "cab5e53d-96ab-4684-e804-0c7e9484c540"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Ein Mann in einem weißen Hemd blickt durch ein Fenster einer Metallkonstruktion . \n",
      "\n",
      "gt   : A man wearing a white shirt is looking out a window of a metal construction . \n",
      "\n",
      "pred: A man in a white shirt looks at a window through a marketplace . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnd_idx = random.randint(0,128)\n",
    "greedy_sample(src, trg, rnd_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiYPYQLbtyEa"
   },
   "source": [
    "As is evident, there is some semblance of a translation. Note that we were rather careless about our training process; we trained for just 40 epochs, didn't perform hyperparameter tuning, and didn't perform any metric calculation (perplexity or BLEU score). Overall, the goal of this notebook was just to get comfortable with transforming text data into a form that works with our neural networks and to train a simple model for German to English.\n",
    "\n",
    "One of the drawbacks of the encoder-decoder model we used was the input sentence was transformed to a final state of fixed size (`H_SIZE=256`) regardless of the size of the input sentence. This problem is addressed by the attention mechansim introduced by [Bahdanau et al. 2016](https://arxiv.org/abs/1409.0473). And this attention mechanism is at the heart of the Transformer model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
