{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpYIBU4rQG4t"
      },
      "source": [
        "# Harry Potter and Word2Vec\n",
        "\n",
        "Word2vec is a method for embedding words into a small latent space which captures semantic meaning. The idea is to use a two layer (one hidden layer) fully connected neural network. The first layer of nodes (representing a one hot encoding of the index of some word $w$) connects to a small hidden layer which then connects to an output layer (representing a one hot encoding of the index of a word that appears near word $w$). Through training, we get the small hidden layer to hold the connection between words and their frequent neighbors.\n",
        "\n",
        "\n",
        "In this demo, we'll implement word2vec using *Harry Potter and the Sorcerer's Stone* as our text corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYiA684zOt4F"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "As always, we rely on a lot of libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq8pZAK27DDz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib.patches import Rectangle\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import numpy\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLHxU1tqOyvA"
      },
      "source": [
        "## Loading Text\n",
        "\n",
        "We load the text of Harry Potter from a website and preprocess it. In particular, we only want the chapters of the book and we make all words lowercase. Finally, we remove annoying characters like new lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wz2yu_chQG2z"
      },
      "outputs": [],
      "source": [
        "url1 = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%201%20-%20The%20Philosopher's%20Stone.txt\"\n",
        "url2 = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%202%20-%20The%20Chamber%20of%20Secrets.txt\"\n",
        "url3 = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%203%20-%20The%20Prisoner%20of%20Azkaban.txt\"\n",
        "url4 = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%204%20-%20The%20Goblet%20of%20Fire.txt\"\n",
        "url5 = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%205%20-%20The%20Order%20of%20the%20Phoenix.txt\"\n",
        "url6 = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%206%20-%20The%20Half%20Blood%20Prince.txt\"\n",
        "url7 = \"https://raw.githubusercontent.com/formcept/whiteboard/master/nbviewer/notebooks/data/harrypotter/Book%207%20-%20The%20Deathly%20Hallows.txt\"\n",
        "\n",
        "def read_file(url, start_phrase, end_phrase, remove):\n",
        "  with urllib.request.urlopen(url) as webpage:\n",
        "    text = webpage.read().decode(\"utf8\")\n",
        "    start = text.index(start_phrase)\n",
        "    text = text[start:]\n",
        "    end = text.index(end_phrase)\n",
        "    text = text[:end]\n",
        "  return text.lower().replace('\\n', '').replace('Â¬','').replace(',','').replace('.','').replace(remove, '')\n",
        "\n",
        "text1 = read_file(url1, 'THE BOY WHO LIVED', 'Page | 348', 'Harry Potter and the Philosophers Stone - J.K. Rowling')\n",
        "text2 = read_file(url2, 'THE WORST BIRTHDAY', 'Page | 380', 'Harry Potter and the Chamber of Secrets - J.K. Rowling')\n",
        "text3 = read_file(url3, 'OWL POST', 'Page | 487', 'Harry Potter and the Prisoner of Azkaban - J.K. Rowling')\n",
        "text4 = read_file(url4, 'THE RIDDLE HOUSE', 'Page | 811', 'Harry Potter and the Goblet of Fire - J.K. Rowling')\n",
        "text5 = read_file(url5, 'DUDLEY DEMENTED', 'Page | 1108', 'Harry Potter and the Order of the Phoenix - J.K. Rowling')\n",
        "text6 = read_file(url6, 'THE OTHER MINISTER', 'Page | 730', 'Harry Potter and the Half Blood Prince - J.K. Rowling')\n",
        "text7 = read_file(url7, 'THE DARK LORD ASCENDING', 'Page | 856', 'Harry Potter and the Deathly Hallows - J.K. Rowling')\n",
        "\n",
        "text = text1 + text2 + text3 + text4 + text5 + text6 + text7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTehzSQtPGaZ"
      },
      "source": [
        "## Tokenizing\n",
        "\n",
        "Now we tokenize the text. That is, we turn the string into a list of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJEVyoWf8J73",
        "outputId": "aa8ccd14-163d-4ef0-bcb4-72d7ffc33d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'boy', 'who', 'lived', 'mr', 'and', 'mrs', 'dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much', 'they', 'were', 'the', 'last', 'people', 'you', 'd', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didn', 't', 'hold', 'with', 'such', 'nonsense', 'mr', 'dursley', 'was', 'the', 'director', 'of', 'a', 'firm', 'called', 'grunnings', 'which', 'made', 'drills', 'he', 'was', 'a', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'a', 'very', 'large', 'mustache', 'mrs', 'dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which', 'came', 'in']\n"
          ]
        }
      ],
      "source": [
        "def tokenize(text):\n",
        "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
        "    return pattern.findall(text.lower())\n",
        "  \n",
        "tokens = tokenize(text)\n",
        "num_tokens = len(set(tokens))\n",
        "print(tokens[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HaRAaxLtNlk9",
        "outputId": "a2be2aa3-13bb-4585-aee0-cca1f57fe532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1169433 total tokens and 21938 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "print(f'There are {len(tokens)} total tokens and {num_tokens} unique tokens.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY_eeC-gPcK8"
      },
      "source": [
        "Now we map from each word (token) to a unique index and back. We will use this for the one hot encoding of the word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyw7PkfQ-iAb"
      },
      "outputs": [],
      "source": [
        "def mapping(tokens):\n",
        "  word_to_id = {}\n",
        "  id_to_word = {}\n",
        "\n",
        "  for i, token in enumerate(set(tokens)):\n",
        "    word_to_id[token] = i\n",
        "    id_to_word[i] = token\n",
        "  \n",
        "  return word_to_id, id_to_word\n",
        "\n",
        "WORD_TO_ID, ID_TO_WORD = mapping(tokens)\n",
        "num_tokens = len(set(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNu4q1OsPkG6"
      },
      "source": [
        "The next step is to get the dataset we will use. We do this by iterating through the tokens with a sliding window. For each word $w$, we look at its neighbors $u$ and add a data point $(w,u)$ where we need to predict $u$ on input $w$. For now, we store the data as indices to avoid carrying around a bunch of zeros in the one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0jJf78x_bRh"
      },
      "outputs": [],
      "source": [
        "def convert(x):\n",
        "  return torch.tensor(np.asarray(x)).long()\n",
        "  \n",
        "def process(tokens, window=2, threshold=100):\n",
        "  frequency = {WORD_TO_ID[token]: 0 for token in set(tokens)}\n",
        "  X, y = [], []\n",
        "  for i in range(len(tokens)-1):\n",
        "    index_i = WORD_TO_ID[tokens[i]]\n",
        "    if frequency[index_i] < threshold:\n",
        "      for j in list(range(max(0,i-window), min(len(tokens), i+window+1))):\n",
        "        index_j = WORD_TO_ID[tokens[j]]\n",
        "        if frequency[index_j] < threshold:\n",
        "          X += [index_i]\n",
        "          y += [index_j]\n",
        "          frequency[index_i] += 1\n",
        "          frequency[index_j] += 1\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8s2bBqjXMO3"
      },
      "source": [
        "# Initialization\n",
        "\n",
        "Now we define the custom class we need so we can wrap the nice DataLoader function around our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTdzbQ9nWck0"
      },
      "outputs": [],
      "source": [
        "class DataSet(Dataset):\n",
        "  def __init__(self, tokens):\n",
        "    X,y = process(tokens)\n",
        "    self.X = X\n",
        "    self.y = y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return self.X[idx], self.y[idx] \n",
        "\n",
        "dataloader = DataLoader(DataSet(tokens), batch_size = 128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI-iOCP0UivK"
      },
      "outputs": [],
      "source": [
        "class Word2Vec(nn.Module):\n",
        "  def __init__(self, num_tokens, embed_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.embedding = nn.Embedding(num_tokens, embed_dim)\n",
        "    self.linear = nn.Linear(embed_dim, num_tokens)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    hidden = self.linear(embedded)\n",
        "    return F.log_softmax(hidden, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_W0DM6ZXQuXR"
      },
      "source": [
        "Finally, we're ready to initialize the architecture, loss, and optimizer. Notice the simplicity of the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5bwwa809pd5"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "embed_dim = 64\n",
        "\n",
        "model = Word2Vec(num_tokens, embed_dim).to(DEVICE)\n",
        "\n",
        "losses = []\n",
        "\n",
        "crossentropy = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-kIbSDIXGqF"
      },
      "source": [
        "## Training\n",
        "\n",
        "Code to interface with google drive for saving and loading model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jKHt6A1x4mZ",
        "outputId": "822948d0-66fc-4d4b-ce95-77e8d705354f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive') # run once\n",
        "path = '/content/gdrive/My Drive/embeddings.pth'\n",
        "#model.load_state_dict(torch.load(path, map_location=DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zlic9nmfD2tC",
        "outputId": "34c28f39-533b-4c50-f8de-ced4c68c47c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \t Loss: 6.246865272521973\n",
            "Epoch: 2 \t Loss: 6.703299522399902\n",
            "Epoch: 3 \t Loss: 4.875844955444336\n",
            "Epoch: 4 \t Loss: 3.78432297706604\n",
            "Epoch: 5 \t Loss: 3.6321003437042236\n",
            "Epoch: 6 \t Loss: 3.599649429321289\n",
            "Epoch: 7 \t Loss: 3.5878701210021973\n",
            "Epoch: 8 \t Loss: 3.2613821029663086\n",
            "Epoch: 9 \t Loss: 2.8468496799468994\n",
            "Epoch: 10 \t Loss: 3.6325862407684326\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  total_loss = 0\n",
        "  for (X,y) in dataloader:\n",
        "    X = X.to(DEVICE)\n",
        "    y = y.to(DEVICE)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X)\n",
        "    loss = crossentropy(outputs, y) \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item()\n",
        "  losses += [total_loss/len(dataloader)]\n",
        "  print(f'Epoch: {epoch} \\t Loss: {loss}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), path)"
      ],
      "metadata": {
        "id": "K2t9SoGotBPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8x_ziNQHRAVt"
      },
      "source": [
        "## Exploring the Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70TH9Lcej1SF"
      },
      "source": [
        "What is the next word predicted?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB3kOnsbkJHA",
        "outputId": "e7615274-ad6d-472e-a5ce-8583a3e6f3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: dumbledore\n",
            "Top 10 related words:\n",
            "dumbledore\n",
            "albus\n",
            "said\n",
            "we\n",
            "dead\n",
            "told\n",
            "toad\n",
            "life\n",
            "re\n",
            "gently\n"
          ]
        }
      ],
      "source": [
        "index = 0\n",
        "words = ['dumbledore']\n",
        "X = torch.tensor([WORD_TO_ID[word] for word in words]).to(DEVICE)\n",
        "print('Word:', ID_TO_WORD[X[index].item()])\n",
        "output = model(X).cpu().detach().numpy()\n",
        "sorted_indices = np.argsort(output[index])[::-1]\n",
        "print('Top 10 related words:')\n",
        "for i in range(10):\n",
        "  print(ID_TO_WORD[sorted_indices[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oogwe4ErUFNd"
      },
      "source": [
        "To investigate the model, we'll need a measure of similarity and a way of extracting the embedding from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT_s4422gMUw"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "  return np.dot(a,b) / (numpy.linalg.norm(a) * numpy.linalg.norm(b))\n",
        "\n",
        "def embed(word):\n",
        "  # Remember the embedding is stored in the weights of the first layer\n",
        "  # We access this by multiplying the input_dim vector by the input_dim x embed_dim matrix\n",
        "  index = WORD_TO_ID[word]\n",
        "  return (model.embedding.weight[index]).detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X44bU5vDRcCh"
      },
      "source": [
        "Let's play around with some words, and visualize the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "EaSX3SxnQZov",
        "outputId": "5b4e30ea-65d7-4bcf-c95c-c5b902f45765"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "words = ['broom', 'wizard', 'harry']\n",
        "embeddings = [embed(word) for word in words]\n",
        "embeddings += [embed('witch')-embed('woman')+embed('man')]\n",
        "num_embed = len(embeddings)\n",
        "\n",
        "cmap = matplotlib.cm.coolwarm\n",
        "fig, axes = plt.subplots(1,num_embed)\n",
        "norm = matplotlib.colors.Normalize(vmin=embeddings[1].min(), vmax=embeddings[1].max())\n",
        "for ax, embedding in zip(axes, embeddings):\n",
        "  for i in range(embed_dim):\n",
        "    ax.add_patch(Rectangle((0,i/embed_dim),1,.05, color=cmap(norm(embedding[i]))))\n",
        "  ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yeBFGWSjxyk"
      },
      "source": [
        "Let's visualize the similarity between words in a matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUHWoNl8eBH8",
        "outputId": "72fa1e17-72f4-4824-85a3-3478eb101e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sim(broom, wizard) = -0.1572546660900116\n",
            "sim(broom, harry) = -0.03790762275457382\n",
            "sim(wizard, harry) = 0.08466929942369461\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(words)):\n",
        "  word1 = words[i]\n",
        "  for j in range(i+1, len(words)):\n",
        "    word2 = words[j]\n",
        "    print(f'sim({word1}, {word2}) = {cosine_similarity(embed(word1), embed(word2))}')\n",
        "#norm = matplotlib.colors.Normalize(vmin=-1, vmax=1) \n",
        "#fig, axes = plt.subplots(num_embed, num_embed)\n",
        "#for i in range(num_embed):\n",
        "#  for j in range(num_embed):\n",
        "#    ax = axes[i,j]\n",
        "#    similarity = cosine_similarity(embeddings[i], embeddings[j])\n",
        "#    ax.add_patch(Rectangle((0, 0), 1, 1, color=cmap(norm(similarity))))\n",
        "#    ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk1k6ReORjNP"
      },
      "source": [
        "Can we classify characters into Gryffindor and Slytherin using their embeddings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ_POfDhgQ-7",
        "outputId": "62e310c4-b80d-4736-a95a-8dd3f5fe1e07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "person \tgryffindor\tslytherin\n",
            "harry\t0.107601315\t0.073798865\n",
            "ron\t-0.16170725\t-0.13011932\n",
            "ginny\t0.17123602\t-0.06939107\n",
            "draco\t0.09989791\t-0.0040249857\n",
            "severus\t-0.117120065\t0.22967824\n",
            "hagrid\t0.1207563\t0.02847264\n",
            "neville\t0.33364892\t0.066469796\n"
          ]
        }
      ],
      "source": [
        "characters = ['harry', 'ron', 'ginny', 'draco', 'severus', 'hagrid', 'neville']\n",
        "houses = ['gryffindor', 'slytherin']\n",
        "print('person \\t' + '\\t'.join(houses))\n",
        "for character in characters:\n",
        "  print(character + '\\t' + '\\t'.join([str(cosine_similarity(embed(house), embed(character))) for house in houses]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe\n",
        "\n",
        "Another method for embedding words is GloVe proposed in \"[GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\". See below for an implementation. Since the method doesn't use a neural network, we manually implement the gradient update."
      ],
      "metadata": {
        "id": "iLil_1ylo_Hg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADX8g8vddUTM"
      },
      "outputs": [],
      "source": [
        "def process_glove(tokens, window=2):\n",
        "  X = np.zeros((num_tokens,num_tokens))\n",
        "  for i in range(len(tokens)):\n",
        "    for j in list(range(max(0,i-window), min(len(tokens), i+window+1))):\n",
        "      index_i = WORD_TO_ID[tokens[i]]\n",
        "      index_j = WORD_TO_ID[tokens[j]]\n",
        "      weight = 2 if (i-j) == 0 else 1/(i-j)**2\n",
        "      X[index_i, index_j] += 1\n",
        "      X[index_j, index_i] += 1\n",
        "  return X\n",
        "\n",
        "class GloVe():\n",
        "  # https://nlp.stanford.edu/pubs/glove.pdf\n",
        "  def __init__(self, tokens, embed_dim):\n",
        "    super(GloVe, self).__init__()\n",
        "    self.X = process_glove(tokens)\n",
        "    self.W = np.random.normal(size=(2*num_tokens, embed_dim))\n",
        "    self.b = np.random.normal(size=2*num_tokens)\n",
        "    # Prevent common pairs from dominating\n",
        "    self.f = lambda x : (x/100)** (3/4) if x < 100 else 1 \n",
        "    self.nonzero_row, self.nonzero_col = np.nonzero(self.X)    \n",
        "  \n",
        "  def train(self, lr):\n",
        "    # Cost function is given by\n",
        "    # Sum_{i,j} f(X[i,j]) * ( w_i^T w_j' + b_i + b_j' - log(X[i,j]) )^2\n",
        "    total_cost = 0\n",
        "    for (i,j) in zip(self.nonzero_row, self.nonzero_col):\n",
        "      main = self.W[i]\n",
        "      context = self.W[j+num_tokens]\n",
        "      fX = self.f(self.X[i,j])\n",
        "      cost = (main @ context + self.b[i+num_tokens] + self.b[j+num_tokens] - np.log(self.X[i,j]))\n",
        "      # f(X[i,j]) * \n",
        "      self.W[i] -= lr * fX * cost * context\n",
        "      self.W[j+num_tokens] -= lr * fX * cost * main\n",
        "      self.b[i] -= lr * fX * cost\n",
        "      self.b[j+num_tokens] -= lr * fX * cost\n",
        "      total_cost += fX * cost **2\n",
        "    return total_cost/len(self.nonzero_row)\n",
        "\n",
        "def train(num_epochs):\n",
        "  costs = []\n",
        "  for epoch in range(num_epochs):\n",
        "    costs += [model.train(lr=.01)]\n",
        "    print(f'Epoch: {epoch} \\t Cost: {costs[-1]}')\n",
        "  return costs\n",
        "\n",
        "def embed_glove(word):\n",
        "  index = WORD_TO_ID[word]\n",
        "  return model.W[index]\n",
        "\n",
        "# Most similar words using GloVe\n",
        "def similar_words_glove(word):\n",
        "  i = WORD_TO_ID[word]\n",
        "  similarities = [cosine_similarity(model.W[i], model.W[j+num_tokens]) for j in range(num_tokens)]\n",
        "  for index in np.argsort(similarities)[:10]:\n",
        "    print(ID_TO_WORD[index])\n",
        "\n",
        "#embed_dim = 100\n",
        "#model = GloVe(tokens, embed_dim=embed_dim)\n",
        "#costs = train(num_epochs=100)\n",
        "#similar_words_glove('harry')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}