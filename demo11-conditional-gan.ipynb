{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GANs\n",
    "\n",
    "Based on the excellent tutorial [here](https://github.com/Yangyangii/GAN-Tutorial/blob/master/MNIST/Conditional-GAN.ipynb).\n",
    "\n",
    "By formulating the process as a two-player game, Generative Adversarial Networks (GANs) can be very effective in generating realistic content. However, we may want to have more control over what is generated. Conditional GANs offer more control by letting us specify the *class* of output we want. Then we hand the generated content and the class it's supposed to be to the discriminator. The disciminator attempts to differentiate between the generated content of a certain class and the real content of a certain class.\n",
    "\n",
    "The original paper that described conditional GANs is [here](https://arxiv.org/abs/1411.1784)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libaries\n",
    "\n",
    "As always, we load lots of libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib.pyplot import imsave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "For this demo, we will be using the MNIST data set. We can apply GANs to other datasets but the training process takes much longer. Our goal will be to supply random noise and a class label (e.g. a digit between 0 and 9) to the generator and produce an image of that particular digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transform to convert the images to tensor and normalize their RGB values\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5], std=[0.5])]\n",
    ")\n",
    "\n",
    "data = datasets.MNIST(root='../data/', train=True, transform=transform, download=True)\n",
    "\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "We'll need several helper functions for training the conditional GAN. The first converts labels to one hot encoded vectors, we will use it to pass the desired label to the generator. The second will plot a grid of 10x10 images from the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_onehot(x, num_classes=10):\n",
    "    assert isinstance(x, int) or isinstance(x, (torch.LongTensor, torch.cuda.LongTensor))\n",
    "    if isinstance(x, int):\n",
    "        c = torch.zeros(1, num_classes).long()\n",
    "        c[0][x] = 1\n",
    "    else:\n",
    "        x = x.cpu()\n",
    "        c = torch.LongTensor(x.size(0), num_classes)\n",
    "        c.zero_()\n",
    "        c.scatter_(1, x, 1) # dim, index, src value\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_onehot(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_image(G, DEVICE, n_noise=100):\n",
    "    img = np.zeros([280, 280])\n",
    "    for j in range(10):\n",
    "        c = torch.zeros([10, 10]).to(DEVICE)\n",
    "        c[:, j] = 1\n",
    "        z = torch.randn(10, n_noise).to(DEVICE)\n",
    "        y_hat = G(z,c).view(10, 28, 28)\n",
    "        result = y_hat.cpu().data.numpy()\n",
    "        img[j*28:(j+1)*28] = np.concatenate([x for x in result], axis=-1)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "We now instantiate the generator and discriminator architectures. The generator takes a random noise vector and a one hot encoded label as input and produces an image. The discriminator takes an image and a one hot encoded label as input and produces a single value between 0 and 1. The discriminator is trained to output 1 for real images and 0 for fake images. The generator is trained to fool the discriminator by outputting images that look real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size=100, num_classes=10, image_size=28*28):\n",
    "        super(Generator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size+num_classes, 128), # auxillary dimension for label\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, image_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat((x, c), 1) # v: [input, label] concatenated vector\n",
    "        y_ = self.network(v)\n",
    "        y_ = y_.view(x.size(0), 1, 28, 28)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=28*28, num_classes=10, num_output=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size+num_classes, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, num_output),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, c):        \n",
    "        x, c = x.view(x.size(0), -1), c.view(c.size(0), -1).float()\n",
    "        v = torch.cat((x, c), 1) # v: [input, label] concatenated vector\n",
    "        y_ = self.network(v)\n",
    "        return y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and Training\n",
    "\n",
    "Now, we're ready to instantiate our models, hyperparameters, and optimizers. Since the task is so easy for MNIST, we will train for only 10 epochs. We will update the generator and discriminator in every step but often one can be trained more frequently than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'ConditionalGAN'\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "D = Discriminator().to(DEVICE)\n",
    "G = Generator().to(DEVICE)\n",
    "\n",
    "max_epoch = 10\n",
    "step = 0\n",
    "n_noise = 100 # size of noise vector\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "G_opt = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# We will denote real images as 1s and fake images as 0s\n",
    "# This is why we needed to drop the last batch of the data loader\n",
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator label: real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label: fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 2, 7, 7, 3, 8, 1, 0, 1, 1])\n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(data_loader))\n",
    "y = labels.view(batch_size, 1)\n",
    "y = to_onehot(y).to(DEVICE)\n",
    "print(labels[:10])\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Step: 0, D Loss: 1.3584825992584229, G Loss: -0.7258844971656799\n",
      "Epoch: 0/10, Step: 500, D Loss: 1.1766839027404785, G Loss: -0.64305180311203\n",
      "Epoch: 1/10, Step: 1000, D Loss: 1.296534776687622, G Loss: -0.5812564492225647\n",
      "Epoch: 1/10, Step: 1500, D Loss: 1.2279163599014282, G Loss: -0.563061535358429\n",
      "Epoch: 2/10, Step: 2000, D Loss: 1.1567275524139404, G Loss: -0.612327516078949\n",
      "Epoch: 2/10, Step: 2500, D Loss: 1.247572660446167, G Loss: -0.4299231171607971\n",
      "Epoch: 3/10, Step: 3000, D Loss: 1.2804317474365234, G Loss: -0.5676645040512085\n",
      "Epoch: 3/10, Step: 3500, D Loss: 1.2663471698760986, G Loss: -0.6986215114593506\n",
      "Epoch: 4/10, Step: 4000, D Loss: 1.1879297494888306, G Loss: -0.5735682845115662\n",
      "Epoch: 4/10, Step: 4500, D Loss: 1.360691785812378, G Loss: -0.7579827904701233\n",
      "Epoch: 5/10, Step: 5000, D Loss: 1.3730287551879883, G Loss: -0.8057775497436523\n",
      "Epoch: 5/10, Step: 5500, D Loss: 1.358701229095459, G Loss: -0.621470034122467\n",
      "Epoch: 6/10, Step: 6000, D Loss: 1.2673840522766113, G Loss: -0.5113570094108582\n",
      "Epoch: 6/10, Step: 6500, D Loss: 1.3537209033966064, G Loss: -0.6859030723571777\n",
      "Epoch: 7/10, Step: 7000, D Loss: 1.3172531127929688, G Loss: -0.7329458594322205\n",
      "Epoch: 8/10, Step: 7500, D Loss: 1.3222179412841797, G Loss: -0.5955038070678711\n",
      "Epoch: 8/10, Step: 8000, D Loss: 1.274102807044983, G Loss: -0.5591683387756348\n",
      "Epoch: 9/10, Step: 8500, D Loss: 1.3625569343566895, G Loss: -0.6081657409667969\n",
      "Epoch: 9/10, Step: 9000, D Loss: 1.351792335510254, G Loss: -0.5453694462776184\n"
     ]
    }
   ],
   "source": [
    "# a directory to save the generated images\n",
    "if not os.path.exists('samples'):\n",
    "    os.makedirs('samples')\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, labels) in enumerate(data_loader):\n",
    "        # Training Discriminator\n",
    "        x = images.to(DEVICE)\n",
    "        y = labels.view(batch_size, 1) # add singleton dimension so batch_size x 1\n",
    "        y = to_onehot(y).to(DEVICE)\n",
    "        x_outputs = D(x, y) # input includes labels\n",
    "        D_x_loss = criterion(x_outputs, D_labels) # Discriminator loss for real images\n",
    "\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y) # input to both generator and discriminator includes labels\n",
    "        D_z_loss = criterion(z_outputs, D_fakes) # Discriminator loss for fake images\n",
    "        D_loss = D_x_loss + D_z_loss # Total Discriminator loss\n",
    "        \n",
    "        D.zero_grad()\n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        # Training Generator\n",
    "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
    "        z_outputs = D(G(z, y), y)\n",
    "        G_loss = -1 * criterion(z_outputs, D_fakes) # Generator loss is negative disciminator loss\n",
    "\n",
    "        G.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_opt.step()\n",
    "        \n",
    "        if step % 500 == 0:\n",
    "            print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {}'.format(epoch, max_epoch, step, D_loss.item(), G_loss.item()))\n",
    "        \n",
    "        if step % 1000 == 0:\n",
    "            G.eval()\n",
    "            img = get_sample_image(G, DEVICE, n_noise)\n",
    "            imsave('samples/{}_step{}.jpg'.format(MODEL_NAME, str(step).zfill(3)), img, cmap='gray')\n",
    "            G.train()\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ee71ea249aa2e9e4602de52516e559983ad773b5ebbcec62edf843d39d54f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
